# ============================================================
# 01_Dedup_pipe_CI_2.7.py
# ============================================================
# ì‹¤í–‰ íë¦„:
#   1) ROOT / BASE ê²½ë¡œ ì…ë ¥ ë° í™•ì¸
#   2) ëª¨ë“œ ì„ íƒ:
#        1 = DUP ëª¨ë“œ     (ì¤‘ë³µ ê·¸ë£¹ TOP N)
#        2 = BIGFILE ëª¨ë“œ (ëŒ€ìš©ëŸ‰ ì¤‘ë³µ ê·¸ë£¹ TOP N)
#   3) ìƒì„± ìˆ˜ëŸ‰ N ì…ë ¥ (TOP N ê·¸ë£¹ ê°œìˆ˜)
#   4) BASE\Runs\run_YYYYMMDD_HHMM ìƒì„±  â† ì´ˆ ë‹¨ìœ„ ì œê±°
#        - DUP    : 01_review_dup/ ì´í•˜ ê·¸ë£¹ í´ë” + .lnk
#        - BIGFILE: 01_review_big/ ì´í•˜ ê·¸ë£¹ í´ë” + .lnk
#   5) run_meta.txt ê¸°ë¡ (ROOT/BASE/RUN_ID/MODE/TOP_N ë“±)
#   6) 02 ì‹¤í–‰ìš© cmd / í…ìŠ¤íŠ¸ ìƒì„±
#
# ëª¨ë“œë³„ ë™ì‘:
#   [DUP]
#     1) ì‚¬ì´ì¦ˆ â†’ blake3 â†’ SHA256 ì¤‘ë³µ í›„ë³´ íƒì§€
#     2) ê·¸ë£¹ ë¦¬í¬íŠ¸ ìƒì„±
#     3) COUNT>=3 í•„í„°
#     4) wasted_bytes ê¸°ì¤€ TOP N ê·¸ë£¹ ì„ íƒ
#     5) review ë§í¬ ìƒì„±
#
#   [BIGFILE]
#     1) min_size_mb ì´ìƒ(+ í™•ì¥ì í•„í„°) íŒŒì¼ë§Œ í›„ë³´
#     2) size â†’ blake3 â†’ SHA256ìœ¼ë¡œ ì¤‘ë³µ ê·¸ë£¹(>=2ê°œ) íƒì§€
#     3) wasted_bytes ê¸°ì¤€ TOP N ê·¸ë£¹ ì„ íƒ
#     4) review ë§í¬ ìƒì„±
#
#   ì‚­ì œ/ì´ë™ ì—†ìŒ. ë¦¬ë·°/í›„ë³´ë§Œ ìƒì„±.
# ============================================================

import os
import csv
import re
import time
import hashlib
import subprocess
from pathlib import Path
from collections import defaultdict
from datetime import datetime

try:
    from blake3 import blake3
except Exception as e:
    raise SystemExit(
        "blake3 ëª¨ë“ˆì´ í•„ìš”í•¨. ì•„ë˜ë¡œ ì„¤ì¹˜ í›„ ì¬ì‹¤í–‰:\n"
        r"  C:\elice\venv\Scripts\python.exe -m pip install blake3"
    ) from e

# ê¸°ë³¸ ê²½ë¡œ
DEFAULT_ROOT = r"C:\Users\Meclaser\Desktop\Mec_DB"
DEFAULT_BASE = r"C:\elice\dedup"

CHUNK = 1024 * 1024

# DUP ëª¨ë“œìš©
MIN_COUNT = 3

# BIGFILE ëª¨ë“œ ê¸°ë³¸ê°’ë“¤
BIG_MIN_SIZE_MB = 200
BIG_EXT_WHITELIST = [
    ".mp4", ".mov", ".mkv", ".avi",
    ".wmv", ".flv", ".mpg", ".mpeg",
]
# BIGFILE ëª¨ë“œì—ì„œ ì¤‘ë³µìœ¼ë¡œ ì¸ì •í•  ìµœì†Œ ê°œìˆ˜
BIG_MIN_DUP_COUNT = 2

PRINT_EVERY_FILES = 5000
PRINT_EVERY_HASH = 1000

WIN_PATH_RE = re.compile(r"""[A-Za-z]:\\[^\r\n\|\;\,"]+""")

# 02 ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì´ë¦„ (ë‚˜ì¤‘ì— ë²„ì „ ë°”ê¾¸ë©´ ì—¬ê¸°ë§Œ ìˆ˜ì •)
NEXT_02_SCRIPT = "02_Full_pipe_CI_2.7.py"


def stamp(msg: str):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}", flush=True)


def prompt_dir(prompt: str, default: str, must_exist: bool, create_if_missing: bool) -> Path:
    while True:
        s = input(f"{prompt}\n(default: {default})\n> ").strip()
        if not s:
            s = default
        p = Path(s)

        if must_exist:
            if not p.exists():
                print(f"[ERR] ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {p}")
                continue
            if not p.is_dir():
                print(f"[ERR] í´ë”ê°€ ì•„ë‹˜: {p}")
                continue
            return p

        if (not p.exists()) and create_if_missing:
            try:
                p.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                print(f"[ERR] í´ë” ìƒì„± ì‹¤íŒ¨: {p} ({e})")
                continue
        return p


# ---------------- ê³µí†µ í•´ì‹œ í•¨ìˆ˜ ----------------

def fast_hash(path: str) -> str:
    h = blake3()
    with open(path, "rb") as f:
        for c in iter(lambda: f.read(CHUNK), b""):
            h.update(c)
    return h.hexdigest()


def sha256_hex(path: str) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for c in iter(lambda: f.read(CHUNK), b""):
            h.update(c)
    return h.hexdigest()


# ---------------- DUP ëª¨ë“œ: ì¤‘ë³µ íƒì§€ ----------------

def step1_scan_duplicates(ROOT: Path, CSV_DUP: Path):
    stamp("STEP 1/5: íŒŒì¼ í¬ê¸° ìˆ˜ì§‘ ì‹œì‘")
    t0 = time.time()

    size_groups = defaultdict(list)
    scanned = 0
    failed = 0

    for root, _, files in os.walk(str(ROOT)):
        for name in files:
            p = os.path.join(root, name)
            scanned += 1
            try:
                size_groups[os.path.getsize(p)].append(p)
            except Exception:
                failed += 1

            if scanned % PRINT_EVERY_FILES == 0:
                stamp(f"  scanned={scanned:,} failed={failed:,} size_buckets={len(size_groups):,}")

    candidates = {k: v for k, v in size_groups.items() if len(v) > 1}
    cand_files = sum(len(v) for v in candidates.values())
    stamp(
        "STEP 1/5: í¬ê¸° í›„ë³´ ì¶”ì¶œ ì™„ë£Œ "
        f"buckets={len(candidates):,} files={cand_files:,} "
        f"(elapsed={time.time()-t0:.1f}s)"
    )

    stamp("STEP 1/5: blake3 í•´ì‹œ ê³„ì‚° ì‹œì‘")
    t1 = time.time()

    fast_groups = defaultdict(list)
    hashed_fast = 0

    for group in candidates.values():
        for p in group:
            try:
                fast_groups[(os.path.getsize(p), fast_hash(p))].append(p)
                hashed_fast += 1
            except Exception:
                pass

            if hashed_fast % PRINT_EVERY_HASH == 0:
                stamp(f"  blake3 hashed={hashed_fast:,} fast_groups={len(fast_groups):,}")

    stamp(f"STEP 1/5: blake3 ì™„ë£Œ (elapsed={time.time()-t1:.1f}s) fast_groups={len(fast_groups):,}")

    stamp("STEP 1/5: SHA256 ìµœì¢… ê²€ì¦ ì‹œì‘")
    t2 = time.time()

    final = defaultdict(list)
    hashed_sha = 0

    for (size, _fh), files in fast_groups.items():
        if len(files) > 1:
            for p in files:
                try:
                    final[(size, sha256_hex(p))].append(p)
                    hashed_sha += 1
                except Exception:
                    pass

                if hashed_sha % PRINT_EVERY_HASH == 0:
                    stamp(f"  sha256 hashed={hashed_sha:,} final_groups={len(final):,}")

    stamp(f"STEP 1/5: SHA256 ì™„ë£Œ (elapsed={time.time()-t2:.1f}s) final_groups={len(final):,}")

    stamp("STEP 1/5: 01_duplicate_result.csv ì €ì¥")
    with CSV_DUP.open("w", newline="", encoding="utf-8-sig") as f:
        w = csv.writer(f)
        w.writerow(["SIZE", "SHA256", "FILE_PATH"])
        out_rows = 0
        for (size, sha), files in final.items():
            if len(files) > 1:
                for p in files:
                    w.writerow([size, sha, p])
                    out_rows += 1

    stamp(
        "STEP 1/5: ì™„ë£Œ -> "
        f"{CSV_DUP} rows={out_rows:,} (total_elapsed={time.time()-t0:.1f}s)"
    )


def human_bytes(n: int) -> str:
    units = ["B", "KB", "MB", "GB", "TB"]
    f = float(n)
    i = 0
    while f >= 1024 and i < len(units) - 1:
        f /= 1024.0
        i += 1
    return f"{int(f)} {units[i]}" if i == 0 else f"{f:.2f} {units[i]}"


def step2_group_report(CSV_DUP: Path, CSV_GROUP: Path, TXT_GROUP: Path):
    stamp("STEP 2/5: grouped_report ìƒì„± ì‹œì‘")
    t0 = time.time()

    groups = defaultdict(list)  # (sha, size) -> [paths]
    rows = 0

    with CSV_DUP.open("r", encoding="utf-8-sig", newline="") as f:
        r = csv.DictReader(f)
        for row in r:
            rows += 1
            try:
                size = int(row["SIZE"])
            except Exception:
                continue

            sha = (row.get("SHA256") or "").strip()
            fp = (row.get("FILE_PATH") or "").strip()
            if sha and fp:
                groups[(sha, size)].append(fp)

            if rows % 20000 == 0:
                stamp(f"  read_rows={rows:,} groups={len(groups):,}")

    dup_groups = [(k, v) for k, v in groups.items() if len(v) > 1]
    stamp(f"  duplicate groups(sha+size)={len(dup_groups):,}")

    with CSV_GROUP.open("w", encoding="utf-8-sig", newline="") as f:
        w = csv.writer(f)
        w.writerow(["SHA256", "SIZE_BYTES", "SIZE_HUMAN", "COUNT", "PATHS"])
        for (sha, size), paths in dup_groups:
            w.writerow([sha, size, human_bytes(size), len(paths), " | ".join(paths)])

    lines = []
    lines.append("Grouped Duplicate Report (by SHA256 + SIZE)\n")
    lines.append(f"Input: {CSV_DUP}\n")
    lines.append(f"Total duplicate groups (sha+size): {len(dup_groups)}\n")
    lines.append("\n")

    for (sha, size), paths in dup_groups:
        header = f"[{size} bytes | {human_bytes(size)} | count={len(paths)} | sha256={sha}]"
        lines.append(header)
        for p in paths:
            lines.append(f"  - {p}")
        lines.append("")

    TXT_GROUP.write_text("\n".join(lines), encoding="utf-8")
    stamp(
        "STEP 2/5: ì™„ë£Œ -> "
        f"{CSV_GROUP}, {TXT_GROUP} (elapsed={time.time()-t0:.1f}s)"
    )


def step3_count_filter(CSV_GROUP: Path, CSV_COUNT3: Path):
    stamp("STEP 3/5: COUNT>=3 í•„í„° ìƒì„± ì‹œì‘")
    t0 = time.time()
    kept = 0

    with CSV_GROUP.open("r", encoding="utf-8-sig", newline="") as f_in, \
         CSV_COUNT3.open("w", encoding="utf-8-sig", newline="") as f_out:

        reader = csv.DictReader(f_in)
        writer = csv.DictWriter(f_out, fieldnames=reader.fieldnames)
        writer.writeheader()

        for r in reader:
            try:
                if int(r.get("COUNT", "0")) >= MIN_COUNT:
                    writer.writerow(r)
                    kept += 1
            except Exception:
                pass

    stamp(
        "STEP 3/5: ì™„ë£Œ -> "
        f"{CSV_COUNT3} rows={kept:,} (elapsed={time.time()-t0:.1f}s)"
    )


def extract_paths(path_blob: str):
    if not path_blob:
        return []
    found = WIN_PATH_RE.findall(str(path_blob).strip())
    seen = set()
    out = []
    for p in found:
        p2 = p.strip().strip('"').strip("'").strip()
        if p2 and p2 not in seen:
            seen.add(p2)
            out.append(p2)
    return out


def step4_big_dup_analysis(CSV_COUNT3: Path, CSV_BIG: Path, CSV_BIG_PATHS: Path, top_n: int):
    """
    DUP ëª¨ë“œ STEP 4/5

    - COUNT >= MIN_COUNT(ê¸°ë³¸ 3) ê·¸ë£¹ë§Œ ëŒ€ìƒ
    - ê° ê·¸ë£¹ì—ì„œ "ê°€ì¥ í° íŒŒì¼ 1ê°œ í¬ê¸°(max_file_bytes)" ê¸°ì¤€ìœ¼ë¡œ TOP N ì„ ì •
    - ì´ ì •ë ¬ ìˆœì„œë¥¼ ê·¸ëŒ€ë¡œ review ê·¸ë£¹ ìˆœì„œì— ë°˜ì˜í•˜ê¸° ìœ„í•´
      CSV_BIG_PATHSë„ top ë¦¬ìŠ¤íŠ¸ ìˆœì„œëŒ€ë¡œ ê¸°ë¡í•œë‹¤.
    """
    stamp("STEP 4/5: í° íŒŒì¼ ê¸°ì¤€ TOP ë¶„ì„ ì‹œì‘")
    t0 = time.time()

    groups = []  # ê° ì›ì†Œ: dict(sha256, count, max_file_bytes, total_bytes, wasted_bytes, keeper_candidate_path, path_sizes)
    read_rows = 0

    with CSV_COUNT3.open("r", encoding="utf-8-sig", newline="") as f:
        reader = csv.DictReader(f)
        need = {"SHA256", "COUNT", "PATHS"}
        if not need.issubset(set(reader.fieldnames or [])):
            raise ValueError(f"Missing columns: need={need}, got={reader.fieldnames}")

        for row in reader:
            read_rows += 1
            sha = (row.get("SHA256") or "").strip()
            if not sha:
                continue

            paths = extract_paths(row.get("PATHS", ""))
            if len(paths) < MIN_COUNT:
                continue  # COUNT í•„í„°

            max_size = -1
            keeper = ""
            total = 0
            path_sizes: list[tuple[str, int]] = []

            for p in paths:
                try:
                    sz = os.path.getsize(p)
                except OSError:
                    sz = 0
                total += sz
                path_sizes.append((p, sz))
                if sz > max_size:
                    max_size = sz
                    keeper = p

            # max_sizeê°€ 0 ì´í•˜ì¸ ê·¸ë£¹(ëª¨ë‘ ì ‘ê·¼ ì‹¤íŒ¨)ì€ ìŠ¤í‚µ
            if max_size <= 0:
                continue

            wasted = total - max_size

            groups.append({
                "sha256": sha,
                "count": len(paths),
                "max_file_bytes": max_size,
                "total_bytes": total,
                "wasted_bytes": wasted,
                "keeper_candidate_path": keeper,
                "path_sizes": path_sizes,
            })

            if read_rows % 5000 == 0:
                stamp(f"  scanned_rows={read_rows:,} kept_groups={len(groups):,}")

    if not groups:
        stamp("STEP 4/5: COUNT/ì‚¬ì´ì¦ˆ ì¡°ê±´ì— ë§ëŠ” ê·¸ë£¹ì´ ì—†ìŠµë‹ˆë‹¤.")
        # ë¹ˆ CSVë¼ë„ ë§Œë“¤ì–´ ë‘”ë‹¤.
        with CSV_BIG.open("w", encoding="utf-8-sig", newline="") as f:
            w = csv.DictWriter(
                f,
                fieldnames=["sha256", "count", "max_file_bytes", "total_bytes", "wasted_bytes", "keeper_candidate_path"],
            )
            w.writeheader()
        with CSV_BIG_PATHS.open("w", encoding="utf-8-sig", newline="") as f:
            w = csv.DictWriter(f, fieldnames=["sha256", "path", "size_bytes"])
            w.writeheader()
        return

    # ğŸ”¹ ì •ë ¬ ê¸°ì¤€ ë³€ê²½: "ê°€ì¥ í° íŒŒì¼ 1ê°œ í¬ê¸°" ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ
    groups.sort(key=lambda g: g["max_file_bytes"], reverse=True)
    top = groups[:top_n]

    # 04_big_dup_top.csv ì‘ì„± (ìš”ì•½ ë©”íƒ€)
    with CSV_BIG.open("w", encoding="utf-8-sig", newline="") as f:
        w = csv.DictWriter(
            f,
            fieldnames=[
                "sha256",
                "count",
                "max_file_bytes",
                "total_bytes",
                "wasted_bytes",
                "keeper_candidate_path",
            ],
        )
        w.writeheader()
        for g in top:
            w.writerow({
                "sha256": g["sha256"],
                "count": g["count"],
                "max_file_bytes": g["max_file_bytes"],
                "total_bytes": g["total_bytes"],
                "wasted_bytes": g["wasted_bytes"],
                "keeper_candidate_path": g["keeper_candidate_path"],
            })

    # 05_big_dup_top_paths.csv ì‘ì„± (ì‹¤ì œ ê²½ë¡œ + í¬ê¸°)
    # ğŸ‘‰ ì—¬ê¸°ì„œë„ top ìˆœì„œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ,
    #    review ê·¸ë£¹ í´ë” ìˆœì„œ = max_file_bytes ê¸°ì¤€ ìˆœì„œê°€ ëœë‹¤.
    with CSV_BIG_PATHS.open("w", encoding="utf-8-sig", newline="") as f:
        w = csv.DictWriter(f, fieldnames=["sha256", "path", "size_bytes"])
        w.writeheader()

        for g in top:  # ì´ë¯¸ max_file_bytes ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ëœ ìƒíƒœ
            sha = g["sha256"]
            for p, sz in g["path_sizes"]:
                w.writerow({"sha256": sha, "path": p, "size_bytes": sz})

    stamp(
        "STEP 4/5: ì™„ë£Œ -> "
        f"{CSV_BIG}, {CSV_BIG_PATHS} groups={len(top):,} (elapsed={time.time()-t0:.1f}s)"
    )


# ---------------- BIGFILE ëª¨ë“œ: ëŒ€ìš©ëŸ‰ "ì¤‘ë³µ" í›„ë³´ ê·¸ë£¹ ----------------

def step_bigfile_candidates(ROOT: Path, CSV_BIG: Path, CSV_BIG_PATHS: Path,
                            min_size_mb: int, max_groups: int,
                            exts=None):
    """
    BIGFILE ëª¨ë“œ:
      - min_size_mb ì´ìƒ(+ í™•ì¥ì í•„í„°)
      - size â†’ blake3 â†’ sha256 ì¤‘ë³µ ê·¸ë£¹(íŒŒì¼ ìˆ˜ >= BIG_MIN_DUP_COUNT)ë§Œ ëŒ€ìƒ
      - wasted_bytes ê¸°ì¤€ TOP max_groups ê·¸ë£¹ ì„ íƒ
    """
    stamp("BIGFILE MODE: ëŒ€ìš©ëŸ‰ ì¤‘ë³µ ê·¸ë£¹ ìˆ˜ì§‘ ì‹œì‘")
    t0 = time.time()

    min_bytes = min_size_mb * 1024 * 1024
    exts_norm = None
    if exts:
        exts_norm = {e.lower() for e in exts}

    size_buckets = defaultdict(list)
    scanned = 0

    for root, _, files in os.walk(str(ROOT)):
        for name in files:
            p = os.path.join(root, name)
            scanned += 1
            try:
                size = os.path.getsize(p)
            except OSError:
                continue

            if size < min_bytes:
                continue

            if exts_norm:
                ext = os.path.splitext(name)[1].lower()
                if ext not in exts_norm:
                    continue

            size_buckets[size].append(p)

            if scanned % PRINT_EVERY_FILES == 0:
                stamp(f"  scanned={scanned:,} big_size_buckets={len(size_buckets):,}")

    # size ê¸°ì¤€ìœ¼ë¡œ 2ê°œ ì´ìƒ ìˆëŠ” ê²ƒë§Œ ë‚¨ê¹€
    size_buckets = {
        sz: paths for sz, paths in size_buckets.items()
        if len(paths) >= BIG_MIN_DUP_COUNT
    }

    if not size_buckets:
        stamp("BIGFILE MODE: ì¡°ê±´ì— ë§ëŠ” 'ëŒ€ìš©ëŸ‰ ì¤‘ë³µ ê·¸ë£¹(size)' ì—†ìŒ.")
        with CSV_BIG.open("w", newline="", encoding="utf-8-sig") as f:
            w = csv.DictWriter(
                f,
                fieldnames=["sha256", "count", "total_bytes", "wasted_bytes", "keeper_candidate_path"]
            )
            w.writeheader()
        with CSV_BIG_PATHS.open("w", newline="", encoding="utf-8-sig") as f:
            w = csv.DictWriter(f, fieldnames=["sha256", "path", "size_bytes"])
            w.writeheader()
        return

    stamp(f"BIGFILE MODE: size ê¸°ì¤€ ì¤‘ë³µ í›„ë³´ ë²„í‚·={len(size_buckets):,}")

    # blake3 1ì°¨
    fast_groups = defaultdict(list)
    hashed_fast = 0

    for size, paths in size_buckets.items():
        for p in paths:
            try:
                h_fast = fast_hash(p)
            except Exception:
                continue
            fast_groups[(size, h_fast)].append(p)
            hashed_fast += 1

            if hashed_fast % PRINT_EVERY_HASH == 0:
                stamp(f"  blake3 hashed={hashed_fast:,} fast_groups={len(fast_groups):,}")

    # sha256 ìµœì¢…
    final_groups = defaultdict(list)
    hashed_sha = 0

    for (size, h_fast), paths in fast_groups.items():
        if len(paths) < BIG_MIN_DUP_COUNT:
            continue
        for p in paths:
            try:
                h_sha = sha256_hex(p)
            except Exception:
                continue
            final_groups[(size, h_sha)].append(p)
            hashed_sha += 1

            if hashed_sha % PRINT_EVERY_HASH == 0:
                stamp(f"  sha256 hashed={hashed_sha:,} final_groups={len(final_groups):,}")

    metrics = []
    for (size, sha), paths in final_groups.items():
        if len(paths) < BIG_MIN_DUP_COUNT:
            continue

        total_bytes = 0
        keeper = ""
        max_sz = -1

        for p in paths:
            try:
                sz = os.path.getsize(p)
            except OSError:
                sz = size
            total_bytes += sz
            if sz > max_sz:
                max_sz = sz
                keeper = p

        wasted = total_bytes - (max_sz if max_sz > 0 else 0)

        metrics.append({
            "sha256": sha,
            "count": len(paths),
            "total_bytes": total_bytes,
            "wasted_bytes": wasted,
            "keeper_candidate_path": keeper,
        })

    if not metrics:
        stamp("BIGFILE MODE: í•´ì‹œ ê¸°ì¤€ìœ¼ë¡œ ë‚¨ëŠ” ëŒ€ìš©ëŸ‰ ì¤‘ë³µ ê·¸ë£¹ì´ ì—†ìŒ.")
        with CSV_BIG.open("w", newline="", encoding="utf-8-sig") as f:
            w = csv.DictWriter(
                f,
                fieldnames=["sha256", "count", "total_bytes", "wasted_bytes", "keeper_candidate_path"]
            )
            w.writeheader()
        with CSV_BIG_PATHS.open("w", newline="", encoding="utf-8-sig") as f:
            w = csv.DictWriter(f, fieldnames=["sha256", "path", "size_bytes"])
            w.writeheader()
        return

    metrics.sort(key=lambda x: x["wasted_bytes"], reverse=True)
    top = metrics[:max_groups]
    top_sha = {m["sha256"] for m in top}

    with CSV_BIG.open("w", newline="", encoding="utf-8-sig") as f:
        w = csv.DictWriter(
            f,
            fieldnames=["sha256", "count", "total_bytes", "wasted_bytes", "keeper_candidate_path"]
        )
        w.writeheader()
        w.writerows(top)

    with CSV_BIG_PATHS.open("w", newline="", encoding="utf-8-sig") as f:
        w = csv.DictWriter(f, fieldnames=["sha256", "path", "size_bytes"])
        w.writeheader()
        for (size, sha), paths in final_groups.items():
            if sha not in top_sha:
                continue
            for p in paths:
                try:
                    sz = os.path.getsize(p)
                except OSError:
                    sz = size
                w.writerow({"sha256": sha, "path": p, "size_bytes": sz})

    stamp(
        "BIGFILE MODE: ì™„ë£Œ -> "
        f"{CSV_BIG}, {CSV_BIG_PATHS} groups={len(top):,} (elapsed={time.time()-t0:.1f}s)"
    )


# ---------------- ë¦¬ë·° ë§í¬ ìƒì„± (ê·¸ë£¹ í´ë” ë„˜ë²„ë§) ----------------

def safe_filename(s: str, max_len=180) -> str:
    s = re.sub(r'[<>:"/\\|?*\x00-\x1F]', "_", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s[:max_len].rstrip() if len(s) > max_len else s


def short_label_from_path(p: Path) -> str:
    parts = p.parts
    drive = parts[0].replace(":", "") if len(parts) > 0 else "DRV"
    parents = p.parent.parts[-2:] if len(p.parent.parts) >= 2 else p.parent.parts
    parent_str = "__".join([x for x in parents if x and x not in (p.drive, "\\")])
    base = p.name
    label = f"{drive}__{parent_str}__{base}" if parent_str else f"{drive}__{base}"
    return safe_filename(label)


def format_size_tag(sz: int) -> str:
    if sz >= 1024**3:
        return f"{sz / (1024**3):.1f}GB"
    if sz >= 1024**2:
        return f"{sz / (1024**2):.3f}MB"
    return f"{max(1, sz // 1024)}KB"


def create_shortcut_lnk(link_path: Path, target_path: Path):
    lnk = str(link_path).replace("'", "''")
    tgt = str(target_path).replace("'", "''")

    ps = (
        "$WshShell = New-Object -ComObject WScript.Shell;\n"
        f"$Shortcut = $WshShell.CreateShortcut('{lnk}');\n"
        f"$Shortcut.TargetPath = '{tgt}';\n"
        f"$Shortcut.WorkingDirectory = (Split-Path '{tgt}' -Parent);\n"
        "$Shortcut.Save();\n"
    )

    subprocess.run(
        ["powershell", "-NoProfile", "-ExecutionPolicy", "Bypass", "-Command", ps],
        check=True,
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL
    )


def step5_make_review_links(CSV_BIG_PATHS: Path, REVIEW_DIR: Path):
    # ì…ë ¥ CSV: sha256(=ê·¸ë£¹ID), path, size_bytes
    #  - DUP ëª¨ë“œ   : sha256 = ì‹¤ì œ ì¤‘ë³µ ê·¸ë£¹ í•´ì‹œ
    #  - BIGFILE ëª¨ë“œ: sha256 = "BIG_0001" ê°™ì€ ê°€ì§œ ê·¸ë£¹ ID ë˜ëŠ” ì‹¤ì œ í•´ì‹œ
    # ê·¸ë£¹ í´ë” ì´ë¦„: 01_SHA_xxx, 02_SHA_xxx ...
    stamp(f"STEP 5: review ë§í¬ ìƒì„± ì‹œì‘ -> {REVIEW_DIR}")
    t0 = time.time()

    groups = defaultdict(list)

    with CSV_BIG_PATHS.open("r", encoding="utf-8-sig", newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            gid = (row.get("sha256") or "").strip()
            p = (row.get("path") or "").strip()
            if not gid or not p:
                continue
            try:
                sz = int(row.get("size_bytes") or 0)
            except Exception:
                sz = 0
            groups[gid].append((p, sz))

    made_groups = 0
    made_links = 0
    missing_targets = 0

    ordered = list(groups.items())  # dict ì‚½ì… ìˆœì„œ ìœ ì§€

    for idx, (gid, items) in enumerate(ordered, start=1):
        sha_tag = gid[:32] if len(gid) > 32 else gid
        group_dir = REVIEW_DIR / f"{idx:02d}_SHA_{sha_tag}"
        group_dir.mkdir(parents=True, exist_ok=True)
        made_groups += 1

        items.sort(key=lambda x: x[1], reverse=True)

        for file_idx, (p_str, sz) in enumerate(items, start=1):
            target = Path(p_str)
            if not target.exists():
                missing_targets += 1

            label = short_label_from_path(target)
            size_tag = format_size_tag(sz)

            link_name = f"{file_idx:02d}__{size_tag}__{label}.lnk"
            link_path = group_dir / safe_filename(link_name, max_len=220)

            if link_path.exists():
                continue

            create_shortcut_lnk(link_path, target)
            made_links += 1

            if made_links % 500 == 0:
                stamp(f"  shortcuts_created={made_links:,} groups={made_groups:,}")

    stamp(f"STEP 5: ì™„ë£Œ (elapsed={time.time()-t0:.1f}s)")
    stamp(f"- Review root: {REVIEW_DIR}")
    stamp(f"- Groups created: {made_groups:,}")
    stamp(f"- Shortcuts created: {made_links:,}")
    stamp(f"- Missing target files: {missing_targets:,}")


# ---------------- ëª¨ë“œ/ê°œìˆ˜ ì…ë ¥ ----------------

def prompt_mode() -> str:
    while True:
        print()
        print("[ëª¨ë“œ ì„ íƒ]")
        print("  1) DUP ëª¨ë“œ     (ì¤‘ë³µ ê·¸ë£¹ TOP N)")
        print("  2) BIGFILE ëª¨ë“œ (ëŒ€ìš©ëŸ‰ ì¤‘ë³µ ê·¸ë£¹ TOP N)")
        s = input("ì„ íƒ (Enter=2 BIGFILE): ").strip()

        if not s:
            return "BIGFILE"
        if s == "1":
            return "DUP"
        if s == "2":
            return "BIGFILE"

        s_up = s.upper()
        if s_up in ("DUP", "BIGFILE"):
            return s_up

        print("ì˜ëª»ëœ ì…ë ¥. 1 / 2 / DUP / BIGFILE ì¤‘ í•˜ë‚˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")


def prompt_top_n(default_n: int = 50) -> int:
    while True:
        s = input(f"ìƒì„±í•  ê·¸ë£¹ ê°œìˆ˜ N (Enter={default_n}): ").strip()
        if not s:
            return default_n
        try:
            n = int(s)
            if n <= 0:
                print("Nì€ 1 ì´ìƒì˜ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
                continue
            return n
        except ValueError:
            print("ì •ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")


# ---------------- 02 ì‹¤í–‰ìš© cmd ìƒì„± ----------------

def write_next_02_cmd(BASE: Path, RUN_DIR: Path, sample_n: int = 10):
    # RUN_DIR í•˜ìœ„ì—:
    #   - run_02_next.cmd
    #   - NEXT_02_CMD.txt
    # ë¥¼ ìƒì„±í•˜ê³ , ì½˜ì†”ì—ë„ ë™ì¼í•œ ëª…ë ¹ì„ ì¶œë ¥í•œë‹¤.
    base_str = str(BASE)
    run_str = str(RUN_DIR)

    cmd_lines = [
        "@echo off",
        f'cd /d "{base_str}"',
        f'py {NEXT_02_SCRIPT} "{run_str}" {sample_n}',
        "pause",
        "",
    ]
    cmd_text = "\n".join(cmd_lines)

    cmd_path = RUN_DIR / "run_02_next.cmd"
    txt_path = RUN_DIR / "NEXT_02_CMD.txt"

    try:
        cmd_path.write_text(cmd_text, encoding="utf-8")
    except Exception as e:
        stamp(f"[WARN] run_02_next.cmd ì‘ì„± ì‹¤íŒ¨: {e}")

    try:
        txt_path.write_text(
            f'cd /d "{base_str}" && py {NEXT_02_SCRIPT} "{run_str}" {sample_n}\n',
            encoding="utf-8",
        )
    except Exception as e:
        stamp(f"[WARN] NEXT_02_CMD.txt ì‘ì„± ì‹¤íŒ¨: {e}")

    print()
    print("[NEXT: run 02]")
    print(f'cd /d "{base_str}"')
    print(f'py {NEXT_02_SCRIPT} "{run_str}" {sample_n}')
    print()


# ---------------- main ----------------

def main():
    stamp("BOOT: starting...")

    ROOT = prompt_dir(
        prompt="ëŒ€ìƒ í´ë”(ROOT)ë¥¼ ì…ë ¥ (Enter=ê¸°ë³¸ê°’)",
        default=DEFAULT_ROOT,
        must_exist=True,
        create_if_missing=False
    )

    BASE = prompt_dir(
        prompt="ê²°ê³¼ í´ë”(BASE)ë¥¼ ì…ë ¥ (Enter=ê¸°ë³¸ê°’, ì—†ìœ¼ë©´ ìƒì„±)",
        default=DEFAULT_BASE,
        must_exist=False,
        create_if_missing=True
    )

    # 1) ë¨¼ì € RUN_IDë§Œ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë§Œë“  ë’¤
    RUN_ID = datetime.now().strftime("%Y%m%d_%H%M")

    # 2) ëª¨ë“œ/ê°œìˆ˜ë¶€í„° ì„ íƒ
    print()
    stamp(f"RUN_ID = {RUN_ID}")
    stamp(f"ROOT   = {ROOT}")
    stamp(f"BASE   = {BASE}")
    print("ìœ„ ê²½ë¡œ/í´ë” êµ¬ì„±ì´ ë§ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. (ì˜ëª»ë˜ì—ˆìœ¼ë©´ Ctrl+Cë¡œ ì¤‘ë‹¨)")

    mode = prompt_mode()
    top_n = prompt_top_n(default_n=50)

    # 3) ëª¨ë“œì— ë”°ë¼ run í´ë” ì´ë¦„ì— suffix ë¶€ì—¬
    #    ì˜ˆ: run_20260126_0553_dup, run_20260126_0553_big
    if mode == "DUP":
        mode_suffix = "dup"
        review_sub = "01_review_dup"
    else:
        mode_suffix = "big"
        review_sub = "01_review_big"

    RUNS_ROOT = BASE / "Runs"
    RUNS_ROOT.mkdir(parents=True, exist_ok=True)

    RUN_DIR = RUNS_ROOT / f"run_{RUN_ID}_{mode_suffix}"
    REVIEW_DIR = RUN_DIR / review_sub

    RUN_DIR.mkdir(parents=True, exist_ok=True)
    REVIEW_DIR.mkdir(parents=True, exist_ok=True)

    # ê¸°ë³¸ CSV ê²½ë¡œ (ì´ë¦„ì€ ê·¸ëŒ€ë¡œ)
    CSV_DUP = RUN_DIR / "01_duplicate_result.csv"
    CSV_GROUP = RUN_DIR / "02_grouped_report.csv"
    TXT_GROUP = RUN_DIR / "02_grouped_report.txt"
    CSV_COUNT3 = RUN_DIR / "03_count_3_plus.csv"
    CSV_BIG = RUN_DIR / "04_big_dup_top.csv"
    CSV_BIG_PATHS = RUN_DIR / "05_big_dup_top_paths.csv"

    # run_meta.txt ê¸°ë¡
    meta_path = RUN_DIR / "run_meta.txt"
    try:
        with meta_path.open("w", encoding="utf-8") as f:
            f.write(f"ROOT={ROOT}\n")
            f.write(f"BASE={BASE}\n")
            f.write(f"RUN_ID={RUN_ID}\n")
            f.write(f"MODE={mode}\n")
            f.write(f"TOP_N={top_n}\n")
    except Exception as e:
        stamp(f"[WARN] run_meta.txt ê¸°ë¡ ì‹¤íŒ¨: {e}")

    stamp(f"RUN_DIR = {RUN_DIR}")
    stamp(f"MODE    = {mode}")
    stamp(f"TOP_N   = {top_n}")
    stamp(f"RUN_DIR = {RUN_DIR}")
    stamp(f"MODE    = {mode}")
    stamp(f"TOP_N   = {top_n}")

    # ---------------- ì‹¤ì œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ----------------
    if mode == "DUP":
        stamp("=== DUP ëª¨ë“œ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
        # 1) ì „ì²´ ì¤‘ë³µ íƒì§€
        step1_scan_duplicates(ROOT, CSV_DUP)
        # 2) ê·¸ë£¹ ë¦¬í¬íŠ¸
        step2_group_report(CSV_DUP, CSV_GROUP, TXT_GROUP)
        # 3) COUNT>=3 í•„í„°
        step3_count_filter(CSV_GROUP, CSV_COUNT3)
        # 4) wasted_bytes ê¸°ì¤€ TOP N ê·¸ë£¹ ì„ íƒ
        step4_big_dup_analysis(CSV_COUNT3, CSV_BIG, CSV_BIG_PATHS, top_n)
        # 5) ë¦¬ë·° ë§í¬ ìƒì„±
        step5_make_review_links(CSV_BIG_PATHS, REVIEW_DIR)
    else:
        stamp("=== BIGFILE ëª¨ë“œ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
        # ëŒ€ìš©ëŸ‰ ì¤‘ë³µ ê·¸ë£¹ í›„ë³´ ìˆ˜ì§‘ + TOP N
        step_bigfile_candidates(
            ROOT,
            CSV_BIG,
            CSV_BIG_PATHS,
            BIG_MIN_SIZE_MB,
            top_n,
            BIG_EXT_WHITELIST,
        )
        # ë¦¬ë·° ë§í¬ ìƒì„±
        step5_make_review_links(CSV_BIG_PATHS, REVIEW_DIR)

    # 02 ì‹¤í–‰ìš© cmd / í…ìŠ¤íŠ¸ ìƒì„±
    write_next_02_cmd(BASE, RUN_DIR, sample_n=10)



if __name__ == "__main__":
    main()

